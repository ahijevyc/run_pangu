{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19bd4c-64ae-43cc-8ef0-780569e61fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pdb\n",
    "from pathlib import Path\n",
    "from typing import Any, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# --- MONKEY-PATCH TQDM FOR JUPYTER ---\n",
    "# This must be done BEFORE importing any library that uses tqdm\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "from hwt_mpas import MPASDataSource\n",
    "from netCDF4 import Dataset\n",
    "from tqdm import notebook\n",
    "\n",
    "tqdm.tqdm = notebook.tqdm\n",
    "\n",
    "from earth2studio.data import GEFS_FX, GFS, DataSource, LandSeaMask, SurfaceGeoPotential\n",
    "from earth2studio.io import IOBackend, NetCDF4Backend\n",
    "from earth2studio.io.zarr import ZarrBackend\n",
    "from earth2studio.models.px import GraphCastOperational, GraphCastSmall\n",
    "from earth2studio.run import deterministic\n",
    "from earth2studio.utils.type import CoordSystem\n",
    "\n",
    "SCRATCH = Path(os.getenv(\"SCRATCH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1258cc-1174-4b34-ab1d-36c02782f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryDataSource(DataSource):\n",
    "    \"\"\"A simple data source that holds a single xarray.DataArray state in memory.\"\"\"\n",
    "\n",
    "    def __init__(self, data: xr.DataArray):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __call__(self, init_time, variable, **kwargs):\n",
    "        return self.data\n",
    "\n",
    "\n",
    "# Define a custom IO class that subsets the data before writing to NetCDF.\n",
    "class SubsetNetCDF4Backend(IOBackend):\n",
    "    def __init__(\n",
    "        self, file_name: str, lat_slice: slice, lon_slice: slice, backend_kwargs: dict = {}\n",
    "    ):\n",
    "        self.file_name = file_name\n",
    "        self.lat_slice = lat_slice\n",
    "        self.lon_slice = lon_slice\n",
    "        self.backend_kwargs = backend_kwargs\n",
    "        self.writer = None\n",
    "\n",
    "    def add_array(\n",
    "        self, coords: CoordSystem, array_name: str | list[str], **kwargs: dict[str, Any]\n",
    "    ) -> None:\n",
    "        # Create a temporary xarray object to correctly select coordinate values.\n",
    "        dummy_data = np.zeros([len(v) for v in coords.values()])\n",
    "        temp_da = xr.DataArray(dummy_data, coords=coords, dims=list(coords.keys()))\n",
    "\n",
    "        # Select the subset using coordinate values (degrees)\n",
    "        subset_da = temp_da.sel(lat=self.lat_slice, lon=self.lon_slice)\n",
    "\n",
    "        # Extract the subsetted coordinates as a dictionary\n",
    "        subset_coords = {k: v.values for k, v in subset_da.coords.items()}\n",
    "\n",
    "        # Initialize the internal NetCDF4Backend with the subsetted coordinates\n",
    "        self.writer = NetCDF4Backend(self.file_name, self.backend_kwargs)\n",
    "        self.writer.add_array(subset_coords, array_name, **kwargs)\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        x: torch.Tensor | list[torch.Tensor],\n",
    "        coords: CoordSystem,\n",
    "        array_name: str | list[str],\n",
    "    ) -> None:\n",
    "        if self.writer is None:\n",
    "            raise RuntimeError(\"add_array must be called before write.\")\n",
    "\n",
    "        if not isinstance(x, list):\n",
    "            x = [x]\n",
    "            array_name = [array_name]\n",
    "\n",
    "        for i, tensor in enumerate(x):\n",
    "            var_name = array_name[i]\n",
    "\n",
    "            # 1. Create a DataArray from the incoming global data tensor\n",
    "            temp_da = xr.DataArray(tensor.cpu().numpy(), coords=coords, dims=list(coords.keys()))\n",
    "\n",
    "            # 2. Select the desired subset using coordinate values (degrees)\n",
    "            subset_da = temp_da.sel(lat=self.lat_slice, lon=self.lon_slice)\n",
    "\n",
    "            # 3. Extract the subsetted data and coordinates for the writer\n",
    "            subset_x = torch.from_numpy(subset_da.data)\n",
    "            subset_coords = {k: v.values for k, v in subset_da.coords.items()}\n",
    "\n",
    "            # 4. Write the subsetted data using the internal writer\n",
    "            self.writer.write(subset_x, subset_coords, var_name)\n",
    "\n",
    "    def close(self):\n",
    "        if self.writer:\n",
    "            self.writer.close()\n",
    "\n",
    "\n",
    "def run(init_time, model, z, lsm, members=[\"gec00\"] + [f\"gep{p:02d}\" for p in range(1, 31)]):\n",
    "    forecast_length = 240\n",
    "    forecast_step_hours = 6\n",
    "    nsteps = forecast_length // forecast_step_hours\n",
    "\n",
    "    output_dir = f\"/glade/derecho/scratch/ahijevyc/ai-models/output/graphcast/{init_time:%Y%m%d%H}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Ensemble forecast outputs will be saved in: {output_dir}\")\n",
    "\n",
    "    model_variables = model.input_coords()[\"variable\"]\n",
    "    vars_to_zero_fill = [v for v in model_variables if v.startswith(\"w\") or v == \"tp06\"]\n",
    "\n",
    "    vars_to_fetch = [v for v in model_variables if v not in vars_to_zero_fill]\n",
    "    vars_to_fetch.remove(\"z\")\n",
    "    vars_to_fetch.remove(\"lsm\")\n",
    "\n",
    "    for member in members:\n",
    "        output_filepath = os.path.join(output_dir, f\"{member}.nc\")\n",
    "        lat_slice = slice(20, 60)\n",
    "        lon_slice = slice(220, 300)\n",
    "\n",
    "        if os.path.exists(output_filepath):\n",
    "            try:\n",
    "                with xr.open_dataset(output_filepath) as ds:\n",
    "                    if len(ds.data_vars) != 85:\n",
    "                        raise ValueError(\n",
    "                            f\"Incorrect number of data variables. Expected 85, found {len(ds.data_vars)}.\"\n",
    "                        )\n",
    "                    for dim_name, dim_size in ds.dims.items():\n",
    "                        if dim_size == 0:\n",
    "                            raise ValueError(f\"Dimension '{dim_name}' has size 0.\")\n",
    "                    if any(ds.z500.squeeze().max(dim=[\"lat\", \"lon\"]) > 1e30):\n",
    "                        raise ValueError(f\"bad data in {output_filepath}\")\n",
    "                    print(\n",
    "                        f\"Valid and complete forecast file already exists for member '{member}', skipping.\"\n",
    "                    )\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Found invalid or incomplete file for member '{member}', removing. Error: {e}\"\n",
    "                )\n",
    "                os.remove(output_filepath)\n",
    "\n",
    "        print(f\"Fetching initial conditions for {member} at {init_time.isoformat()}...\")\n",
    "        gefs_source = GEFS_FX(member=member)\n",
    "        initial_state_partial = gefs_source(init_time, [datetime.timedelta(hours=0)], vars_to_fetch)\n",
    "\n",
    "        print(f\"Regridding initial state for {member}...\")\n",
    "        lat = model.input_coords()[\"lat\"]\n",
    "        lon = model.input_coords()[\"lon\"]\n",
    "        wrapped = initial_state_partial.sel(lon=0).assign_coords(lon=360)\n",
    "        initial_state_periodic = xr.concat([initial_state_partial, wrapped], dim=\"lon\")\n",
    "        initial_state_partial = initial_state_periodic.interp(lat=lat, lon=lon, method=\"linear\")\n",
    "\n",
    "        data_arrays_to_concat = [initial_state_partial]\n",
    "        for var_name in vars_to_zero_fill:\n",
    "            zero_array = xr.zeros_like(initial_state_partial.isel(variable=0))\n",
    "            zero_array[\"variable\"] = var_name\n",
    "            data_arrays_to_concat.append(zero_array)\n",
    "        data_arrays_to_concat.extend([z, lsm])\n",
    "        initial_state = xr.concat(data_arrays_to_concat, dim=\"variable\", coords=\"minimal\")\n",
    "\n",
    "        assert initial_state.notnull().all()\n",
    "        initial_state = initial_state.sel(variable=model_variables).squeeze(\n",
    "            dim=\"lead_time\", drop=True\n",
    "        )\n",
    "\n",
    "        in_memory_source = MemoryDataSource(initial_state)\n",
    "\n",
    "        print(f\"Running forecast and subsetting for member '{member}'...\")\n",
    "\n",
    "        subset_writer = SubsetNetCDF4Backend(\n",
    "            file_name=output_filepath,\n",
    "            lat_slice=lat_slice,\n",
    "            lon_slice=lon_slice,\n",
    "            backend_kwargs={\"mode\": \"w\"},\n",
    "        )\n",
    "\n",
    "        deterministic([init_time], nsteps, model, in_memory_source, subset_writer)\n",
    "\n",
    "        subset_writer.close()\n",
    "\n",
    "        print(f\"Successfully created forecast file: {output_filepath}\")\n",
    "        print(f\"--- Finished forecast for member '{member}' ---\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nâœ… All ensemble member forecasts for {init_time.date()} have been successfully generated.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d8de97-98cf-4521-9e7e-c7141a091591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Block ---\n",
    "print(\"Initializing GraphCast model...\")\n",
    "model_class = GraphCastOperational\n",
    "model = model_class.load_model(model_class.load_default_package())\n",
    "print(\"Model initialized successfully.\")\n",
    "\n",
    "# Load static data (z, lsm), fetching and saving as local files if not present.\n",
    "print(\"Loading static data (z, lsm)...\")\n",
    "lat = model.input_coords()[\"lat\"]\n",
    "lon = model.input_coords()[\"lon\"]\n",
    "static_data_dir = \"/glade/derecho/scratch/ahijevyc/ai-models/static_data\"\n",
    "os.makedirs(static_data_dir, exist_ok=True)\n",
    "z_filepath = os.path.join(static_data_dir, \"graphcast_z_0.25deg.nc\")\n",
    "lsm_filepath = os.path.join(static_data_dir, \"graphcast_lsm_0.25deg.nc\")\n",
    "static_data_time = datetime.datetime(2023, 1, 1)\n",
    "\n",
    "# Handle Geopotential (z)\n",
    "if os.path.exists(z_filepath):\n",
    "    print(f\"Loading z from local file: {z_filepath}\")\n",
    "    z = xr.open_dataarray(z_filepath)\n",
    "else:\n",
    "    print(f\"Fetching z and saving to: {z_filepath}\")\n",
    "    z_data = (\n",
    "        SurfaceGeoPotential(cache=False)([static_data_time])\n",
    "        .sel(lat=lat, lon=lon)\n",
    "        .squeeze(dim=\"time\")\n",
    "    )\n",
    "    z_data[\"variable\"] = [\"z\"]\n",
    "    z_data.to_netcdf(z_filepath)\n",
    "    z = z_data\n",
    "\n",
    "# Handle Land-Sea Mask (lsm)\n",
    "if os.path.exists(lsm_filepath):\n",
    "    print(f\"Loading lsm from local file: {lsm_filepath}\")\n",
    "    lsm = xr.open_dataarray(lsm_filepath)\n",
    "else:\n",
    "    print(f\"Fetching lsm and saving to: {lsm_filepath}\")\n",
    "    lsm_data = (\n",
    "        LandSeaMask(cache=False)([static_data_time]).sel(lat=lat, lon=lon).squeeze(dim=\"time\")\n",
    "    )\n",
    "    lsm_data.to_netcdf(lsm_filepath)\n",
    "    lsm = lsm_data\n",
    "\n",
    "print(\"Static data loaded successfully.\")\n",
    "\n",
    "for init_time in pd.to_datetime(pd.date_range(\"20230424\", \"20230531\")):\n",
    "    print(f\"\\n{'='*20} Starting Run for {init_time.date()} {'='*20}\")\n",
    "    run(init_time, model, z, lsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1122ad-8dfd-4e3f-9a1b-e249a3951a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GFSFill(GFS):\n",
    "    \"\"\"\n",
    "    Intercepts requests for specified variables and provides predefined data arrays.\n",
    "    For all other variables, it falls back to the standard GFS implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, custom_arrays: dict[str, xr.DataArray], *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes with a dictionary of custom xarray.DataArrays,\n",
    "        where the keys are the variable names.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.custom_arrays = custom_arrays\n",
    "        self.custom_vars = list(custom_arrays.keys())\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        time: np.ndarray,\n",
    "        variable: np.ndarray,\n",
    "    ) -> xr.DataArray:\n",
    "        # Identify which custom variables are in the current request\n",
    "        requested_custom_vars = [v for v in self.custom_vars if v in variable]\n",
    "\n",
    "        # Identify which variables need to be fetched from the standard GFS source\n",
    "        other_vars = [v for v in variable if v not in self.custom_vars]\n",
    "\n",
    "        # Fetch data for non-custom variables from the parent GFS class\n",
    "        if other_vars:\n",
    "            gfs_data = super().__call__(time, np.array(other_vars))\n",
    "        else:\n",
    "            gfs_data = None\n",
    "\n",
    "        # Prepare a list of all data arrays to be concatenated\n",
    "        data_to_concat = []\n",
    "        if gfs_data is not None:\n",
    "            data_to_concat.append(gfs_data)\n",
    "\n",
    "        # Add the requested custom arrays to the list\n",
    "        for var_name in requested_custom_vars:\n",
    "            # Expand dims to match the structure of the fetched data\n",
    "            custom_da = self.custom_arrays[var_name].expand_dims({\"time\": time})\n",
    "            data_to_concat.append(custom_da)\n",
    "\n",
    "        # Concatenate all data arrays along the 'variable' dimension\n",
    "        if len(data_to_concat) > 1:\n",
    "            return xr.concat(data_to_concat, dim=\"variable\")\n",
    "        elif len(data_to_concat) == 1:\n",
    "            return data_to_concat[0]\n",
    "        else:\n",
    "            # Should not happen if 'variable' is never empty\n",
    "            return xr.DataArray()\n",
    "\n",
    "# Compare GFS init with and without surface geopotential and land mask\n",
    "init_time = pd.Timestamp(\"20240501\")\n",
    "\n",
    "model = GraphCastOperational.load_model(GraphCastOperational.load_default_package())\n",
    "ds = GFS()\n",
    "# cache=False to avoid AttributeError: type object 'WholeFileCacheFileSystem'\n",
    "# has no attribute '_cat_file'. Did you mean: 'cat_file'?\n",
    "# dummy time list for required positional argument 'time'\n",
    "# squeeze 'time' to avoid ValueError: Dimension time already exists.\n",
    "zsl = SurfaceGeoPotential(cache=False)([0]).squeeze(dim=\"time\")\n",
    "lsm = LandSeaMask(cache=False)([0]).squeeze(dim=\"time\")\n",
    "\n",
    "# --- Instantiate Custom Data Source ---\n",
    "custom_data = {\"zsl\": zsl, \"lsm\": lsm}\n",
    "ds_filled = GFSFill(custom_arrays=custom_data)\n",
    "\n",
    "# --- Run Forecast ---\n",
    "nsteps = 8\n",
    "ofile = SCRATCH / \"GFS.nc\"\n",
    "if os.path.exists(ofile):\n",
    "    os.remove(ofile)\n",
    "io = NetCDF4Backend(ofile, backend_kwargs={\"mode\": \"w\"})\n",
    "deterministic([init_time], nsteps, model, ds, io)\n",
    "io.close()\n",
    "ofile = SCRATCH / \"GFSFill.nc\"\n",
    "if os.path.exists(ofile):\n",
    "    os.remove(ofile)\n",
    "io = NetCDF4Backend(ofile, backend_kwargs={\"mode\": \"w\"})\n",
    "deterministic([init_time], nsteps, model, ds_filled, io)\n",
    "io.close()\n",
    "\n",
    "print(\"Forecast run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4f832-943a-4f40-b98d-ca016b4e17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds(init_time, model.input_coords()[\"variable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ecefb3-1d7e-4f43-a42e-df1c4e4e6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = pd.Timestamp(\"20230502\")\n",
    "model = GraphCastOperational.load_model(GraphCastOperational.load_default_package())\n",
    "mpas_data_source = MPASDataSource(\n",
    "    grid_path=Path(\"MPAS/15km_mesh/grid_mesh/x1.2621442.grid.nc\"),\n",
    "    data_dir=f\"HWT{init_time.year}/mpas_15km\",\n",
    ")\n",
    "ds = mpas_data_source(init_time, 0, members=[1], variables=model.input_coords()[\"variable\"]).squeeze(dim=\"member\")\n",
    "ds = ds.transpose(\"time\",...)\n",
    "ds.loc[:, \"lsm\"] = lsm.values\n",
    "ds.loc[:, \"z\"] = zsl.values\n",
    "\n",
    "ic = MemoryDataSource(ds)\n",
    "nsteps = 8\n",
    "ofile = SCRATCH / \"graphcast_mpas_{init_time:%Y%m%d%H}.nc\"\n",
    "if os.path.exists(ofile):\n",
    "    os.remove(ofile)\n",
    "io = NetCDF4Backend(ofile, backend_kwargs={\"mode\": \"w\"})\n",
    "deterministic([init_time], nsteps, model, ic, io)\n",
    "io.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae205916-365e-4f9e-9bba-d49497e2ef9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5b142-d61f-4904-9f45-05b4a1e56919",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(variable=\"lsm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6b596-c17d-4f82-bf9a-b8ac9945635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = SCRATCH/f\"ai-models/output/graphcast/{init_time:%Y%m%d%H}/gep01.nc\"\n",
    "print(ifile)\n",
    "ds = xr.open_dataset(ifile)\n",
    "ds.z500.squeeze().max(dim=[\"lat\", \"lon\"]) < 1e30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444ae09-bee0-454d-8ac0-299e273c5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(SCRATCH/f\"ai-models/output/graphcast/{init_time:%Y%m%d%H}/gep01.nc\")\n",
    "ds.z100.squeeze().max(dim=[\"lat\",\"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68be4b-3ee5-46c8-85db-e22417b1c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(SCRATCH/f\"ai-models/output/graphcast/{init_time:%Y%m%d%H}/gep23\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc7b14-d25e-4778-8b5f-d2f3ac4dcba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.squeeze().max(dim=[\"lat\",\"lon\"]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee1fbb-21e6-4371-b40d-221c005f2cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv (Earth2Studio)",
   "language": "python",
   "name": "e2s_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

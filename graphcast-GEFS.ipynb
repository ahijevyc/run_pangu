{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19bd4c-64ae-43cc-8ef0-780569e61fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# This must be done BEFORE importing any library that uses tqdm\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "from hwt_mpas import MemoryDataSource, SubsetNetCDF4Backend\n",
    "from tqdm import notebook\n",
    "\n",
    "tqdm.tqdm = notebook.tqdm\n",
    "\n",
    "from earth2studio.data import GEFS_FX, GFS, LandSeaMask, SurfaceGeoPotential\n",
    "from earth2studio.io import NetCDF4Backend\n",
    "from earth2studio.models.px import GraphCastOperational, GraphCastSmall\n",
    "from earth2studio.run import deterministic\n",
    "\n",
    "SCRATCH = Path(os.getenv(\"SCRATCH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1258cc-1174-4b34-ab1d-36c02782f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(init_time, model, z, lsm, members=[\"gec00\"] + [f\"gep{p:02d}\" for p in range(1, 31)]):\n",
    "    forecast_length = 240\n",
    "    forecast_step_hours = 6\n",
    "    nsteps = forecast_length // forecast_step_hours\n",
    "\n",
    "    output_dir = f\"/glade/derecho/scratch/ahijevyc/ai-models/output/graphcast/{init_time:%Y%m%d%H}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Ensemble forecast outputs will be saved in: {output_dir}\")\n",
    "\n",
    "    model_variables = model.input_coords()[\"variable\"]\n",
    "    vars_to_zero_fill = [v for v in model_variables if v.startswith(\"w\") or v == \"tp06\"]\n",
    "\n",
    "    vars_to_fetch = [v for v in model_variables if v not in vars_to_zero_fill]\n",
    "    vars_to_fetch.remove(\"z\")\n",
    "    vars_to_fetch.remove(\"lsm\")\n",
    "\n",
    "    for member in members:\n",
    "        output_filepath = os.path.join(output_dir, f\"{member}.nc\")\n",
    "        lat_slice = slice(20, 60)\n",
    "        lon_slice = slice(220, 300)\n",
    "\n",
    "        if os.path.exists(output_filepath):\n",
    "            try:\n",
    "                with xr.open_dataset(output_filepath) as ds:\n",
    "                    if len(ds.data_vars) != 85:\n",
    "                        raise ValueError(\n",
    "                            f\"Incorrect number of data variables. Expected 85, found {len(ds.data_vars)}.\"\n",
    "                        )\n",
    "                    for dim_name, dim_size in ds.dims.items():\n",
    "                        if dim_size == 0:\n",
    "                            raise ValueError(f\"Dimension '{dim_name}' has size 0.\")\n",
    "                    if any(ds.z500.squeeze().max(dim=[\"lat\", \"lon\"]) > 1e30):\n",
    "                        raise ValueError(f\"bad data in {output_filepath}\")\n",
    "                    print(\n",
    "                        f\"Valid and complete forecast file already exists for member '{member}', skipping.\"\n",
    "                    )\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Found invalid or incomplete file for member '{member}', removing. Error: {e}\"\n",
    "                )\n",
    "                os.remove(output_filepath)\n",
    "\n",
    "        print(f\"Fetching initial conditions for {member} at {init_time.isoformat()}...\")\n",
    "        gefs_source = GEFS_FX(member=member)\n",
    "        initial_state_partial = gefs_source(init_time, [datetime.timedelta(hours=0)], vars_to_fetch)\n",
    "\n",
    "        print(f\"Regridding initial state for {member}...\")\n",
    "        lat = model.input_coords()[\"lat\"]\n",
    "        lon = model.input_coords()[\"lon\"]\n",
    "        wrapped = initial_state_partial.sel(lon=0).assign_coords(lon=360)\n",
    "        initial_state_periodic = xr.concat([initial_state_partial, wrapped], dim=\"lon\")\n",
    "        initial_state_partial = initial_state_periodic.interp(lat=lat, lon=lon, method=\"linear\")\n",
    "\n",
    "        data_arrays_to_concat = [initial_state_partial]\n",
    "        for var_name in vars_to_zero_fill:\n",
    "            zero_array = xr.zeros_like(initial_state_partial.isel(variable=0))\n",
    "            zero_array[\"variable\"] = var_name\n",
    "            data_arrays_to_concat.append(zero_array)\n",
    "        data_arrays_to_concat.extend([z, lsm])\n",
    "        initial_state = xr.concat(data_arrays_to_concat, dim=\"variable\", coords=\"minimal\")\n",
    "\n",
    "        assert initial_state.notnull().all()\n",
    "        initial_state = initial_state.sel(variable=model_variables).squeeze(\n",
    "            dim=\"lead_time\", drop=True\n",
    "        )\n",
    "\n",
    "        in_memory_source = MemoryDataSource(initial_state)\n",
    "\n",
    "        print(f\"Running forecast and subsetting for member '{member}'...\")\n",
    "\n",
    "        subset_writer = SubsetNetCDF4Backend(\n",
    "            file_name=output_filepath,\n",
    "            lat_slice=lat_slice,\n",
    "            lon_slice=lon_slice,\n",
    "            backend_kwargs={\"mode\": \"w\"},\n",
    "        )\n",
    "\n",
    "        deterministic([init_time], nsteps, model, in_memory_source, subset_writer)\n",
    "\n",
    "        subset_writer.close()\n",
    "\n",
    "        print(f\"Successfully created forecast file: {output_filepath}\")\n",
    "        print(f\"--- Finished forecast for member '{member}' ---\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nâœ… All ensemble member forecasts for {init_time.date()} have been successfully generated.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002629ac-b362-43f0-b54e-c8beb9cc9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Block ---\n",
    "model_class = GraphCastSmall\n",
    "print(f\"Initializing {model_class} model...\")\n",
    "model = model_class.load_model(model_class.load_default_package())\n",
    "print(\"Model initialized successfully.\")\n",
    "\n",
    "model.input_coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1be20-1cbf-4482-bc85-be113b0da6ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load static data (z, lsm), fetching and saving as local files if not present.\n",
    "print(\"Loading static data (z, lsm)...\")\n",
    "lat = model.input_coords()[\"lat\"]\n",
    "lon = model.input_coords()[\"lon\"]\n",
    "\n",
    "# Handle Geopotential (z)\n",
    "print(\"Fetching z\")\n",
    "z_data = (\n",
    "    # Tried cache=True but AttributeError: type object 'WholeFileCacheFileSystem' has no attribute '_cat_file'\n",
    "    SurfaceGeoPotential(cache=False)([None])\n",
    "    .sel(lat=lat, lon=lon)\n",
    "    .squeeze()\n",
    ")\n",
    "z = z_data\n",
    "\n",
    "# Handle Land-Sea Mask (lsm)\n",
    "print(\"Fetching lsm\")\n",
    "lsm = LandSeaMask(cache=False)([None]).sel(lat=lat, lon=lon).squeeze()\n",
    "\n",
    "print(\"Static data loaded successfully.\")\n",
    "\n",
    "for init_time in pd.to_datetime(pd.date_range(\"20230424\", \"20230531\")):\n",
    "    print(f\"\\n{'='*20} Starting Run for {init_time.date()} {'='*20}\")\n",
    "    run(init_time, model, z, lsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1122ad-8dfd-4e3f-9a1b-e249a3951a78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GFSWithSfcFields(GFS):\n",
    "    \"\"\"\n",
    "    Intercepts requests for specified variables and provides predefined data arrays.\n",
    "    For all other variables, it falls back to the standard GFS implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, custom_arrays: dict[str, xr.DataArray], *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes with a dictionary of custom xarray.DataArrays,\n",
    "        where the keys are the variable names.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.custom_arrays = custom_arrays\n",
    "        self.custom_vars = list(custom_arrays.keys())\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        time: np.ndarray,\n",
    "        variable: np.ndarray,\n",
    "    ) -> xr.DataArray:\n",
    "        # Identify which custom variables are in the current request\n",
    "        requested_custom_vars = [v for v in self.custom_vars if v in variable]\n",
    "\n",
    "        # Identify which variables need to be fetched from the standard GFS source\n",
    "        other_vars = [v for v in variable if v not in self.custom_vars]\n",
    "\n",
    "        # Fetch data for non-custom variables from the parent GFS class\n",
    "        if other_vars:\n",
    "            gfs_data = super().__call__(time, np.array(other_vars))\n",
    "        else:\n",
    "            gfs_data = None\n",
    "\n",
    "        # Prepare a list of all data arrays to be concatenated\n",
    "        data_to_concat = []\n",
    "        if gfs_data is not None:\n",
    "            data_to_concat.append(gfs_data)\n",
    "\n",
    "        # Add the requested custom arrays to the list\n",
    "        for var_name in requested_custom_vars:\n",
    "            # Expand dims to match the structure of the fetched data\n",
    "            custom_da = self.custom_arrays[var_name].expand_dims({\"time\": time})\n",
    "            data_to_concat.append(custom_da)\n",
    "\n",
    "        # Concatenate all data arrays along the 'variable' dimension\n",
    "        if len(data_to_concat) > 1:\n",
    "            return xr.concat(data_to_concat, dim=\"variable\")\n",
    "        elif len(data_to_concat) == 1:\n",
    "            return data_to_concat[0]\n",
    "        else:\n",
    "            # Should not happen if 'variable' is never empty\n",
    "            return xr.DataArray()\n",
    "\n",
    "\n",
    "# Compare GFS init with and without surface geopotential and land mask\n",
    "init_time = pd.Timestamp(\"20240501\")\n",
    "\n",
    "model = GraphCastOperational.load_model(GraphCastOperational.load_default_package())\n",
    "ds = GFS()\n",
    "# cache=False to avoid AttributeError: type object 'WholeFileCacheFileSystem'\n",
    "# has no attribute '_cat_file'. Did you mean: 'cat_file'?\n",
    "# dummy time list for required positional argument 'time'\n",
    "# squeeze 'time' to avoid ValueError: Dimension time already exists.\n",
    "zsl = SurfaceGeoPotential(cache=False)([0]).squeeze(dim=\"time\")\n",
    "lsm = LandSeaMask(cache=False)([0]).squeeze(dim=\"time\")\n",
    "\n",
    "# --- Instantiate Custom Data Source ---\n",
    "custom_data = {\"zsl\": zsl, \"lsm\": lsm}\n",
    "ds_filled = GFSWithSfcFields(custom_arrays=custom_data)\n",
    "\n",
    "# --- Run Forecast ---\n",
    "nsteps = 8\n",
    "ofile = SCRATCH / \"tmp/GFS.nc\"\n",
    "if os.path.exists(ofile):\n",
    "    os.remove(ofile)\n",
    "io = NetCDF4Backend(ofile, backend_kwargs={\"mode\": \"w\"})\n",
    "deterministic([init_time], nsteps, model, ds, io)\n",
    "io.close()\n",
    "ofile = SCRATCH / \"tmp/GFSWithSfcFields.nc\"\n",
    "if os.path.exists(ofile):\n",
    "    os.remove(ofile)\n",
    "io = NetCDF4Backend(ofile, backend_kwargs={\"mode\": \"w\"})\n",
    "deterministic([init_time], nsteps, model, ds_filled, io)\n",
    "io.close()\n",
    "\n",
    "print(\"Forecast run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4f832-943a-4f40-b98d-ca016b4e17b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds(init_time, model.input_coords()[\"variable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5b142-d61f-4904-9f45-05b4a1e56919",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(variable=\"lsm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6b596-c17d-4f82-bf9a-b8ac9945635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = SCRATCH / f\"ai-models/output/graphcast/{init_time:%Y%m%d%H}/gep01.nc\"\n",
    "print(ifile)\n",
    "ds = xr.open_dataset(ifile)\n",
    "ds.z500.squeeze().max(dim=[\"lat\", \"lon\"]) < 1e30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444ae09-bee0-454d-8ac0-299e273c5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(SCRATCH / f\"ai-models/output/graphcast/{init_time:%Y%m%d%H}/gep01.nc\")\n",
    "ds.z100.squeeze().max(dim=[\"lat\", \"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68be4b-3ee5-46c8-85db-e22417b1c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(SCRATCH / f\"ai-models/output/graphcast/{init_time:%Y%m%d%H}/gep23\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc7b14-d25e-4778-8b5f-d2f3ac4dcba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.squeeze().max(dim=[\"lat\", \"lon\"]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee1fbb-21e6-4371-b40d-221c005f2cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (e2s)",
   "language": "python",
   "name": "earth2studio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
